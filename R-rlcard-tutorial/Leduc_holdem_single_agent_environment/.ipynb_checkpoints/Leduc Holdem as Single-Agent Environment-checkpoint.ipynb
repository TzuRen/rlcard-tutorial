{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "<a href='https://github.com/datamllab/rlcard'> <img src='https://github.com/datamllab/rlcard/raw/master/docs/imgs/logo.jpg' width=500 /></a>\n",
    "___\n",
    "# Example for Leduc Hold'em as Single-Agent Environment in R\n",
    "\n",
    "### Let's train our Leduc Hold'em as Single-Agent Environment in R!\n",
    "\n",
    "\n",
    "We have wrraped the environment as single agent environment by assuming that other players play with pre-trained models. The interfaces are exactly the same to OpenAI Gym. Thus, any single-agent algorithm can be connected to the environment. An example of Leduc Hold'em is as below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## > Install packages\n",
    "To begin with, we install `reticulate`, a package that embeds the Python session within the R session. Specifically, in our tutorial, the `reticulate` is used for virtual environment creation and Python packages connection. Then, we install the package `imager` for the curve plotting of the training performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "install.packages(\"reticulate\")\n",
    "library(reticulate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "install.packages(\"imager\")\n",
    "library(imager)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## > Virtual Environment\n",
    "Now we create a virtual environment called \"r-rlcard\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "virtualenv_create('r-rlcard')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before using the virtual environment `r-rlcard`, let's double-check if it exists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "virtualenv_list()\n",
    "use_virtualenv('r-rlcard', required=TRUE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## >  Import packages\n",
    "First, we use `py_install()` to install `Rlcard` and `Tensorflow` in R. We recommend to use `'pip = TURE'` for installation because the default conda install method may occur issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "py_install('rlcard', pip=TRUE)\n",
    "py_install('rlcard[tensorflow]', pip=TRUE)\n",
    "py_install('numpy', pip=TRUE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we import the installed packages and check if the `TensorFlow` version agrees with our needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rlcard <- import('rlcard')\n",
    "tf <- import('tensorflow')\n",
    "os <- import('os')\n",
    "np <- import('numpy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf$\"__version__\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## > Deep-Q Agent model setup\n",
    "\n",
    "First, we import the modules that are wraped up in rlcard packages. Then, we set the iterations numbers, the initial memory size and how frequently we evaluate the performance. Remember to have `L` to keep the datatype. Finally, we save the performance data and learning curves to our current path as `./log`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the modules.\n",
    "DQNAgent <- rlcard$agents$DQNAgent\n",
    "RandomAgent <- rlcard$agents$RandomAgent\n",
    "set_global_seed <- rlcard$utils$set_global_seed\n",
    "tournament <- rlcard$utils$tournament\n",
    "Logger <- rlcard$utils$Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config <- list(seed = 0L, single_agent_mode = TRUE)\n",
    "env = rlcard$make('leduc-holdem', config)\n",
    "eval_env = rlcard$make('leduc-holdem', config)\n",
    "\n",
    "# Set the iterations numbers and how frequently we evaluate the performance.\n",
    "evaluate_every = 1000L\n",
    "evaluate_num = 10000L\n",
    "timesteps = 20000L\n",
    "\n",
    "# Set the intial memory size.\n",
    "memory_init_size = 1000L\n",
    "\n",
    "# Train the agent every X steps.\n",
    "train_every = 1\n",
    "\n",
    "# Set the paths for saving the logs and learning curves. We save it on our current path.\n",
    "log_dir = './Leduc Holdem_dqn_result/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before starting the training process, we initial a global step and set up the DQN agents. The Logger is used to plot the learning curve and save it to the same directory as we set up for log."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a global seed.\n",
    "set_global_seed(0L)\n",
    "sess <- tf$Session()\n",
    "\n",
    "# Initialize a global step.\n",
    "global_step = tf$Variable(0L, name='global_step', trainable=F)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the DQN agents.\n",
    "agent = DQNAgent(\n",
    "  sess,\n",
    "  scope='dqn',\n",
    "  action_num=env$action_num,\n",
    "  replay_memory_init_size = memory_init_size,\n",
    "  train_every=train_every,\n",
    "  state_shape=env$state_shape,\n",
    "  mlp_layers=c(128, 128)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize global variables and a Logger to plot the learning curve.\n",
    "sess$run(tf$global_variables_initializer())\n",
    "logger = Logger(log_dir)\n",
    "state = env$reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## > Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Training the model requires complicated interactions with Tensorflow. Thus, we recommend importing a Python script. Specifically, we create a file named `train.py` in the same directory with content as follows.\n",
    "\n",
    "\n",
    "    def train(timesteps, action, state, next_state, evaluate_num):\n",
    "       for timestep in range(timesteps):\n",
    "         action = agent.step(state)\n",
    "         next_state, reward, done = env.step(action)\n",
    "         ts = (state, action, reward, next_state, done)\n",
    "         agent.feed(ts)\n",
    "\n",
    "         if timestep % evaluate_every == 0:\n",
    "             rewards = []\n",
    "             state = eval_env.reset()\n",
    "             for _ in range(evaluate_num):\n",
    "                 action, _ = agent.eval_step(state)\n",
    "                 _, reward, done = env.step(action)\n",
    "                 if done:\n",
    "                     rewards.append(reward)\n",
    "             logger.log_performance(env.timestep, np.mean(rewards))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we use the `train.py` to train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reticulate::source_python(\"train_Leduc.py\")\n",
    "train(timesteps, env, eval_env, evaluate_every, evaluate_num, agent, logger, state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Close files in the logger and Plot the learning curve in our file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger$close_files()\n",
    "logger$plot('DQN')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's display our training performace and the plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt <- read.csv(file='./log/performance.csv')\n",
    "txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(imager)\n",
    "load.image(\"~/Desktop/log/fig.png\") %>% plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the end, we save the trained model to `models/leduc_holdem_single_dqn` ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = 'models/leduc_holdem_single_dqn'\n",
    "if (!dir.exists(save_dir)){\n",
    "    os$makedirs(save_dir)}\n",
    "saver = tf$train$Saver()\n",
    "saver$save(sess, os$path$join(save_dir, 'model'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now you have your no-limit Texas Holdem model.\n",
    "## Good Job!\n",
    "\n",
    "___"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
